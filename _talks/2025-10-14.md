---
title: "Fine-tuning LLMs for supervised instruction"
collection: talks
type: "Talk"
permalink: /talks/2025-10-14
venue: "Data Science and Machine Learning Collaborative Learning Group"
date: 2025-10-14
location: "Online"
---
 The talk summarized the process of Instruction Fine-Tuning, which is necessary to convert a pre-trained LLM from a general text completer into a model capable of following specific human commands. The core method involves preparing a supervised dataset of explicit instruction-input-response pairs and then fine-tuning a pre-trained model on this formatted data. The model's weights are adjusted to reliably produce the desired response for a given instruction. The final step is evaluating the model's ability to follow instructions using automated conversational scoring techniques, often by employing another LLM to grade the quality of the generated responses. [Notebook](https://github.com/matthewshawnkehoe/Data-Science-Machine-Learning-Collaborative-Learning-Group/blob/main/LLMs-from-scratch/ch07/ch07.ipynb)