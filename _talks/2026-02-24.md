---
title: "The method of perturbation and curriculum learning in Physics-Informed Neural Networks"
collection: talks
type: "Talk"
permalink: /talks/2026-02-24
venue: "Applied Machine Learning Group"
date: 2026-02-24
location: "Online"
---
This talk introduced the concept of low-rank approximation for tensors and matrices, which utilizes algebraic decompositions like Singular Value Decomposition (SVD) to represent high-dimensional data using a minimal set of essential components. The core method is a geometric approach to dimensionality reduction that identifies eigenvectors and eigenvalues to isolate principal directions of variance within a linear transformation. A visualization-driven intuition for coordinate scaling and space reshaping was also presented, providing information about how high-rank data can be compressed into lower-dimensional subspaces without significant information loss. Overall, low-rank approximations solve challenges in data redundancy and computational complexity, such as in text vectorization (e.g., RETVec) or image processing, more efficiently and robustly than standard full-rank representations. [Notebook](https://github.com/matthewshawnkehoe/Data-Science-Machine-Learning-Collaborative-Learning-Group/blob/main/LoRA/Low_rank_approximation_of_tensors.ipynb).