---
title: "Pretraining on Unlabeled Data and Fine-tuning LLMs for text classification"
collection: talks
type: "Talk"
permalink: /talks/2025-10-14
venue: "Data Science and Machine Learning Collaborative Learning Group"
date: 2025-10-14
location: "Online"
---
 This talk covered the essential post-construction steps of preparing a Large Language Model for practical use, starting with pretraining. This involved training the model on unlabeled data for next-word prediction and introducing text generation techniques like temperature scaling and Top-k sampling. The talk then covered detailed classification fine-tuning, where the model's architecture is modified by adding a classification head and freezing the main weights to specialize it for supervised tasks like spam detection. Together, these concepts illustrate the transformation of a base GPT model into both a creative text generator and a practical text classifier. [Notebook](https://github.com/matthewshawnkehoe/Data-Science-Machine-Learning-Collaborative-Learning-Group/blob/main/LLMs-from-scratch/ch06/ch06.ipynb)